"""Download and remap sequence data from an SRA bioproject

Currently the workflow reads a configuration file that contains
information about bioproject, reference, and more. See
../config/config.yaml for an example. The current format also supports
the definition of regions of interest, from which mapped reads will be
extracted.

Monkeyflower examples have been generated with command

    snakemake -j 14 --profile workflow/local

The workflow/local directory holds a configuration file for a
snakemake profile that sets some useful snakemake options.

## On SRA identifiers

Before any analyses can be run, the workflow will attempt to download
the SRA Run information for the bioproject. This file contains
primarily three identifiers of interest:

- Run: SRA Run accession in the form of SRR########
- Sample: SRA Sample accession in the form of SRS########
- SampleName: Name of sample as provided by submitter

Sample information, such as read group identifiers and output file
names, will be based on these identifiers. We add an additional column
SampleAlias (initially equal to the SampleName column) that will be
used as the sample name in read group assignment.

In case there is a custom.smk file present there must exist a rule
that generates a custom sample information file which is loaded into a
pandas data frame named sampleinfo, and which must have columns
Sample, Run, SampleName, and SampleAlias.

The sample information columns are mapped to the following workflow
variables:

- Run: srarun
- Sample: srasample
- SampleName: samplename
- SampleAlias: sample

The `sample` variable is used for  the shorthand name for
whether it is based on SampleName or SampleAlias.

The relationships between different SRA entities are believed to be:

SRP 1-to-many SRS

SRS 1-to-many SRR

A project consists of many samples, and a sample can be split over
multiple runs. Currently there is no support for samples split over
multiple runs, but this should be handled at the mapping stage, where
samples from different runs are first mapped with correct read group
information, then merged.

## On read groups

ID: unique identifier; on Illumina systems consists of flowcell name
and lane number, so is unique for a srarun.

SM: sample name; could be any of srasample, samplename or sample
(alias).

"""
import os
import sys
import re
import pandas as pd
import shutil
from snakemake.remote.NCBI import RemoteProvider as NCBIRemoteProvider
from snakemake.remote.NCBI import NCBIFileException
from snakemake.remote.HTTP import RemoteProvider
from pathlib import Path
import logging

configfile: "config/config.yaml"

include: "common.smk"

# custom.smk must set sampleinfo object
sampleinfo = None
try:
    include: "custom.smk"
    custom_all=rules.custom_all.input
except Exception as e:
    print(e)
    custom_all=[]
    pass

workdir: config["bioproject"]


envvars:
    "EMAIL"
if "repeatlibrary" in config.keys():
    envvars:
        "REPEATMASKER_LIBDIR"

##############################
# Settings
##############################
# csvtk filter2
csvtk_filter2 = ""
if "sraruninfo" in config.keys():
    if "csvtk.filter2" in config["sraruninfo"].keys():
        csvtk_filter2 = config["sraruninfo"]["csvtk.filter2"]
        csvtk_filter2 = f"-f '{csvtk_filter2}'"
# Data sources
datasources = dict()
if "datasources" in config.keys():
    datasources = dict(zip(config["datasources"].keys(),
                           config["datasources"].values()))
if "repeatlibrary" not in config.keys():
    config["repeatlibrary"] = "repeats"


try:
    sraruninfo = pd.read_csv("SraRunInfo.csv")
    sraruninfo["SampleAlias"] = sraruninfo.SampleName
except Exception as e:
    logging.error("No SraRunInfo.csv available yet! Rerun workflow once it has been downloaded")
    sraruninfo = pd.DataFrame(columns=["Sample", "Run", "SampleName", "SampleAlias"])
    pass


if sampleinfo is None:
    sampleinfo = sraruninfo


NCBI = NCBIRemoteProvider(email=os.environ["EMAIL"])
HTTP = RemoteProvider()

try:
    REPEATMASKER_DIR=os.path.dirname(os.path.realpath(shutil.which("RepeatMasker")))
except Exception as e:
    raise

wildcard_constraints:
    roi="(|" + "|".join([x for x in config.get("output", {}).keys()]) + ")",
    sep="(|/)",
    srrun="(" + "|".join(sampleinfo.Run.values) + ")",
    srsample="(" + "|".join(sampleinfo.Sample.values) + ")",
    samplename="(" + "|".join(sampleinfo.SampleName.values) + ")",
    samplealias="(" + "|".join(sampleinfo.SampleAlias.values) + ")",


all_results = dict(
    sampleinfo=["SraRunInfo.csv"],
    datasources=list(datasources.keys()),
    custom=custom_all,
    masked=expand(f'{config["reference"]}.{{suffix}}', suffix=["masked", "out", "tbl"]),
    srrbam=expand("{samplename}/{srrun}.sort.md.bam.bai", zip, \
                  srrun=sampleinfo.Run.values, \
                  samplename=sampleinfo.SampleName.values)
)


if "repeatlibrary" in config.keys():
    all_results["repeats"] = [f'{config["repeatlibrary"]}-families.fa']
    all_results["merged.lib"] = expand(
        f'{config["reference"]}.{config["repeatlibrary"]}.merged.{{suffix}}',
        suffix=["masked", "out", "tbl"]
    )

if "output" in config.keys():
    for roi, obj in config["output"].items():
        key = f"output.{roi}"
        results = [os.path.join(f"{roi}", "allsites.vcf.gz.tbi"),
                   os.path.join(f"{roi}", "save.sh")]
        results.extend(expand(f"{roi}/{{samplealias}}/{{srrun}}_R1.fastq.gz",
                              zip, \
                              srrun=sampleinfo.Run.values, \
                              samplealias=sampleinfo.SampleAlias.values))
        results.extend(expand(f"{roi}/{{samplealias}}/{{srrun}}_R2.fastq.gz",
                              zip, \
                              srrun=sampleinfo.Run.values, \
                              samplealias=sampleinfo.SampleAlias.values))
        results.append(f"{roi}/multiqc_report.html")
        all_results[key] = results


rule all:
    """Pseudo-rule for all targets"""
    input: **all_results

##############################
# SRR module
#
# Download project data for a bioproject
#
##############################
rule download_sraruninfo:
    """Download sraruninfo for bioproject.

    config["sraruninfo"] configuration section contains configurations
    that apply to this rule.

    csvtk.filter2: set csvtk filter2 filtering parameter
    """
    output:
        "SraRunInfo.csv",
    benchmark: "benchmarks/SraRunInfo.csv.benchmark.txt",
    params:
        bioproject=config["bioproject"],
        filter2=csvtk_filter2,
    conda: "envs/sratools.yaml",
    log: "logs/SraRunInfo.csv.log",
    threads: 1
    shell:
        """
        esearch -db sra -query '{params.bioproject}' |
            efetch -format runinfo |
            csvtk filter2 {params.filter2} > {output}
        """


rule download_datasources:
    """Download datasources listed in config file using wget"""
    output:
        urltarget="{urltarget}",
    input:
        urlsource=lambda wildcards: HTTP.remote(config.get("datasources", {}).get(wildcards.urltarget))
    wildcard_constraints:
        urltarget=f'({"|".join([str(x) for x in set(datasources.keys())])})'
    conda: "envs/environment.yml",
    benchmark: "benchmarks/{urltarget}.benchmark.txt",
    log: "logs/{urltarget}.log",
    threads: 1
    shell:
        """
        wget {input.urlsource} -O {output.urltarget}
        """

##############################
# Reference indexing
##############################
rule samtools_faidx:
    """Run samtools faidx"""
    output: "{prefix}.fasta.fai",
    input: "{prefix}.fasta",
    conda: "envs/environment.yml",
    benchmark: "benchmarks/{prefix}.fasta.fai.benchmark.txt",
    log: "logs/{prefix}.fasta.fai.log",
    threads: 1
    shell:
        """samtools faidx {input}"""

rule picard_create_sequence_dictionary:
    """Create sequence dictionary"""
    output: "{prefix}.dict",
    input: "{prefix}.fasta",
    conda: "envs/environment.yml",
    benchmark: "benchmarks/{prefix}.dict.benchmark.txt",
    log: "logs/{prefix}.dict.log",
    threads: 1
    shell:
        """picard CreateSequenceDictionary --REFERENCE {input} > {log} 2>&1"""


##############################
# Repeat masking
##############################
rule build_database:
    """Build repeatmasker database"""
    output:
        lib=expand("{lib}.{sfx}", lib=config["repeatlibrary"],
                   sfx=["nhr", "nin", "njs", "nnd",
                        "nni", "nog", "nsq",
                        "translation"]),
    input: config["reference"],
    conda: "envs/repeatmasker.yml",
    benchmark: "benchmarks/repeats.benchmark.txt",
    log: "logs/repeats.builddatabase.log",
    threads: 1
    shell:
        """
        BuildDatabase -name {output} -engine ncbi {input} > {log} 2>&1
        """

rule repeat_modeler:
    """Run repeatmodeler"""
    output:
        fa="{lib}-families.fa",
        stk="{lib}-families.stk",
    input:
        lib="{lib}.nhr"
    wildcard_constraints:
        lib=config["repeatlibrary"]
    params:
        lib=config["repeatlibrary"],
    conda: "envs/repeatmasker.yml",
    benchmark: "benchmarks/repeat_modeler.{lib}.benchmark.txt"
    log: "logs/repeat_modeler.{lib}.log"
    threads: 12
    shell:
        """
        RepeatModeler -database {params.lib} -threads {threads} > {log} 2>&1
        """

rule repeat_masker_get_dfam_curated:
    """Install dfam_curated library

    One has to manually download and link a dfam library in the
    RepeatMasker installation location. The RepeatMasker directory is
    located at $(dirname $(readlink -f $(which RepeatMasker))). The
    dfam curated library is small enough to serve the purposes of this
    example.

        wget https://www.dfam.org/releases/Dfam_3.7/families/Dfam_curatedonly.h5.gz -O $(dirname $(readlink -f $(which RepeatMasker)))
    """
    output:
        h5="Libraries/Dfam.h5"
    input:
        dfam_curated=HTTP.remote("https://www.dfam.org/releases/Dfam_3.7/families/Dfam_curatedonly.h5.gz")
    params:
        dfam="Libraries/dfam_curated.h5.gz",
        h5="Libraries/Dfam.h5",
    conda: "envs/repeatmasker.yml",
    benchmark: "benchmarks/repeat_masker/dfam_curated.benchmark.txt"
    log: "logs/repeat_masker/dfam_curated.log"
    threads: 1
    shell:
        """
        wget {input.dfam_curated} -O {params.dfam} > {log} 2>&1
        gzip -dv {params.dfam} >> {log} 2>&1
        mv {params.dfam} {params.h5}
        """

rule repeat_masker:
    """Run repeatmasker on reference with generic repeat library.
    """
    output:
        fasta=f'{config["reference"]}.masked',
        out=f'{config["reference"]}.out',
        tbl=f'{config["reference"]}.tbl',
    input:
        ref=config["reference"],
        dfam="Libraries/Dfam.h5",
    params:
        species=config.get("species", "nn"),
        libdir=os.environ.get("REPEATMASKER_LIBDIR")
    conda: "envs/repeatmasker.yml",
    benchmark: "benchmarks/repeat_masker/masked.fasta.benchmark.txt",
    log: "logs/repeat_masker/masked.fasta.log",
    threads: 14
    shell:
        """
        RepeatMasker -libdir Libraries -libdir {params.libdir}  -species "{params.species}" -gff -pa {threads} -a -xsmall {input.ref} > {log} 2>&1
        """

rule repeat_masker_custom:
    """Run repeatmasker on reference with custom repeat library"""
    output:
        fasta=f'{config["reference"]}.{{lib}}.masked',
        out=f'{config["reference"]}.{{lib}}.out',
        tbl=f'{config["reference"]}.{{lib}}.tbl'
    input:
        ref=f'{config["reference"]}.masked',
        lib="{lib}-families.fa",
    wildcard_constraints:
        lib=config["repeatlibrary"],
    params:
        species=config.get("species", "nn"),
        ref={config["reference"]},
    conda: "envs/repeatmasker.yml",
    benchmark: "benchmarks/repeat_masker/{lib}.masked.fasta.benchmark.txt",
    log: "logs/repeat_masker/{lib}.masked.fasta.log",
    threads: 14
    shell:
        """
        RepeatMasker -lib {input.lib} -gff -pa {threads} -a -xsmall {input.ref} > {log} 2>&1
        rename "s/{params.ref}.masked/{params.ref}.{wildcards.lib}/g" {input.ref}.*
        """

rule repeat_masker_combine:
    """Combine repeatmasker outputs"""
    output:
        out=f'{config["reference"]}.{{lib}}.merged.{{suffix}}',
    input:
        custom=f'{config["reference"]}.{{lib}}.{{suffix}}',
        rm=f'{config["reference"]}.{{suffix}}'
    wildcard_constraints:
        suffix = "(out|tbl|masked)"
    conda: "envs/repeatmasker.yml",
    benchmark: "benchmarks/repeat_masker/{lib}.merged.{suffix}.benchmark.txt",
    log: "logs/repeat_masker/{lib}.merged.{suffix}.log",
    threads: 1
    shell:
        """
        cat {input.rm} {input.custom} > {output.out}
        """

##############################
# Mapping
##############################
BWA_INDEX_SUFFIX=["amb", "ann", "bwt", "pac", "sa"]
rule bwa_index:
    """Create bwa index for input file"""
    output:
        index=expand("{{prefix}}.fasta.{suffix}", suffix=BWA_INDEX_SUFFIX)
    input:
        fasta="{prefix}.fasta",
    conda: "envs/environment.yml",
    benchmark: "benchmarks/{prefix}.fasta.bwa_index.benchmark.txt",
    log: "logs/{prefix}.fasta.bwa_index.log",
    threads: 1
    shell:
        """bwa index {input.fasta} > {log} 2>&1"""

rule sra_prefetch:
    """Prefetch sra record. Note that output name is determined by
    prefetch."""
    output:
        srrun=temp("{srrun}/{srrun}.sra"),
    conda: "envs/environment.yml",
    benchmark: "benchmarks/sra_prefetch/{srrun}.benchmark.txt",
    log: "logs/sra_prefetch/{srrun}.log",
    threads: 1
    shell:
        """
        prefetch -p {output.srrun} > {log} 2>&1
        """

rule bwa_mem_srrinput:
    """Map SRR data on the fly and convert to bam.

    NB: This will use the SampleName column to identify the sample
    (not the SRS id)."""
    output:
        bam="{samplename}/{srrun}.sort.md.bam",
    input:
        srr="{srrun}/{srrun}.sra",
        reference=config["reference"],
        index=expand("{ref}.{sfx}", ref=config["reference"], sfx=BWA_INDEX_SUFFIX),
    params:
        rg=get_read_group,
        options="-M"
    conda: "envs/environment.yml",
    benchmark: "benchmarks/{samplename}/{srrun}.sort.md.bam.benchmark.txt",
    log: "logs/{samplename}/{srrun}.sort.md.bam.log",
    threads: 14
    shell:
        """
        fasterq-dump --skip-technical -Z --split-spot {input.srr} |
            bwa mem {params.rg} -p -t {threads} {params.options} {input.reference} - |
            samtools fixmate -m - /dev/stdout |
            samtools sort - | samtools markdup - /dev/stdout |
            samtools view -h -b -o {output.bam} 2> {log}
        """

rule bwa_mem_ubam_to_roi:
    """Map ubam file to roi"""
    output:
        bam="{roi}/{samplealias}/{srrun}.sort.bam",
    input:
        reference=os.path.join("{roi}", config['reference']),
        index=expand(os.path.join("{{roi}}", "{ref}.{sfx}"), ref=config["reference"], sfx=BWA_INDEX_SUFFIX),
        bam="{roi}/{samplealias}/{srrun}.unmapped.bam",
        bai="{roi}/{samplealias}/{srrun}.unmapped.bam.bai",
    params:
        rg=get_roi_read_group,
        options="-M"
    conda: "envs/environment.yml",
    benchmark: "benchmarks/bwa_mem_roi/{roi}/{samplealias}/{srrun}.bam.benchmark.txt",
    log: "logs/bwa_mem_roi/{roi}/{samplealias}/{srrun}.bam.log",
    threads: 14
    shell:
        """
            samtools fastq {input.bam} 2> {log}|
            bwa mem {params.rg} -p -t {threads} {params.options} {input.reference} - 2>> {log} |
            samtools fixmate -m - /dev/stdout |
            samtools sort - |
            samtools view -h -b -o {output.bam} >> {log} 2>&1
        """


rule samtools_index_bam:
    """Index bam file"""
    output: "{roi}{sep}{samplealias}/{srrun}.{tag}.bam.bai",
    input: "{roi}{sep}{samplealias}/{srrun}.{tag}.bam",
    conda: "envs/environment.yml",
    benchmark: "benchmarks/samtools_index_bam/{roi}{sep}{samplealias}/{srrun}.{tag}.bam.bai.benchmark.txt",
    log: "logs/samtools_index_bam/{srrun}/{roi}{sep}{srrun}.{samplealias}.{tag}.bam.bai.log",
    threads: 1
    shell:
        """
        samtools index {input} > {log} 2>&1
        """


##############################
# Region-based analyses (roi)
#
# 1. Subset reference to roi
#
# 2. Subset bam to roi
#
# 3. Make ubam
#
# 4. Remap ubam to reference
##############################
rule make_roi_bed:
    """Make roi bedfile"""
    output: "{roi}/{roi}.bed",
    params:
        roi = lambda wildcards: "\n".join(["\t".join(re.split(":|-", x)) for x in config["output"][wildcards.roi]["roi"]])
    conda: "envs/environment.yml",
    benchmark: "benchmarks/make_roi_bed/{roi}.bed.benchmark.txt",
    log: "logs/make_roi_bed/{roi}.bed.log",
    threads: 1
    shell:
        """
        echo -e "{params.roi}" > {output}
        """

rule subset_reference_to_roi:
    """Subset reference sequence to roi"""
    output:
        fasta=os.path.join("{roi}", config["reference"]),
    input:
        fasta=config["reference"],
        fai=config["reference"] + ".fai",
        bed="{roi}/{roi}.bed"
    conda: "envs/environment.yml",
    benchmark: "benchmarks/subset_reference_to_roi/{roi}.benchmark.txt",
    log: "logs/subset_reference_to_roi/{roi}.log",
    threads: 1
    shell:
        """
        seqtk subseq -l 60 {input.fasta} {input.bed} |\
        sed -r 's/>([A-Z0-9a-z]+):([0-9\-]+)/>\\1 \\1:\\2/g' > {output.fasta} 2> {log}
        """

rule subset_bam_for_roi:
    """Subset bam file for roi"""
    output:
        bam="{roi}/{samplealias}/{srrun}.roi.bam",
    input:
        bam=lambda wildcards: subset_bam_for_roi_input(wildcards),
        bai=lambda wildcards: subset_bam_for_roi_input(wildcards, ext=".bai"),
    params:
        roi=lambda wildcards: " ".join([f'"{x}"' for x in config["output"][wildcards.roi]["roi"]])
    conda: "envs/environment.yml",
    benchmark: "benchmarks/subset_bam_for_roi/{roi}/{samplealias}/{srrun}.roi.bam.benchmark.txt",
    log: "logs/subset_bam_for_roi/{roi}/{samplealias}/{srrun}.roi.bam.log",
    threads: 1
    shell:
        """
        samtools view -P {input.bam} {params.roi} -h -b -o {output.bam} > {log} 2>&1
        """

rule make_ubam_for_roi:
    """Make ubam file for roi using old sample name"""
    output:
        bam=temp("{roi}/{samplealias}/{srrun}.unmapped.samplealias.bam"),
    input:
        bam="{roi}/{samplealias}/{srrun}.roi.bam",
    conda: "envs/environment.yml",
    benchmark: "benchmarks/make_ubam_for_roi/{roi}/{samplealias}/{srrun}.unmapped.bam.benchmark.txt",
    log: "logs/make_ubam_for_roi/{roi}/{samplealias}/{srrun}.unmapped.bam.log",
    threads: 1
    shell:
        """
        picard RevertSam --INPUT {input.bam} \
                   --OUTPUT {output.bam} \
                   --SANITIZE true \
                   --MAX_DISCARD_FRACTION 0.5 \
                   --ATTRIBUTE_TO_CLEAR XT \
                   --ATTRIBUTE_TO_CLEAR XN \
                   --ATTRIBUTE_TO_CLEAR AS \
                   --ATTRIBUTE_TO_CLEAR OC \
                   --ATTRIBUTE_TO_CLEAR OP \
                   --SAMPLE_ALIAS {wildcards.samplealias} \
                   --SORT_ORDER queryname \
                   --RESTORE_ORIGINAL_QUALITIES true \
                   --REMOVE_DUPLICATE_INFORMATION true \
                   --REMOVE_ALIGNMENT_INFORMATION true \
                   > {log} 2>&1
        """

rule rename_ubam_sample:
    """Rename sample in ubam file"""
    output:
        bam="{roi}/{samplealias}/{srrun}.unmapped.bam",
    input:
        bam="{roi}/{samplealias}/{srrun}.unmapped.samplealias.bam",
    params:
        sample=lambda wildcards: get_srrun_dict(wildcards)["sample"],
    conda: "envs/environment.yml",
    benchmark: "benchmarks/rename_ubam_sample/{roi}/{samplealias}/{srrun}.unmapped.bam.benchmark.txt",
    log: "logs/rename_ubam_sample/{roi}/{samplealias}/{srrun}.unmapped.bam.log",
    threads: 1
    shell:
        """
        samtools view -H {input.bam} | sed "s/SM:[^\t]*/SM:{params.sample}/g" | samtools reheader - {input.bam} > {output.bam}
        """


rule samtools_fastq_roi:
    """Make fastq files from roi"""
    output:
        R1="{roi}/{samplealias}/{srrun}_R1.fastq.gz",
        R2="{roi}/{samplealias}/{srrun}_R2.fastq.gz",
    input:
        bam="{roi}/{samplealias}/{srrun}.unmapped.bam",
        bai="{roi}/{samplealias}/{srrun}.unmapped.bam.bai",
    conda: "envs/environment.yml",
    benchmark: "benchmarks/samtools_fastq_roi/{roi}/{samplealias}/{srrun}.fastq.gz.benchmark.txt",
    log: "logs/samtools_fastq_roi/{roi}/{samplealias}/{srrun}.fastq.gz.log",
    threads: 1
    shell:
        """
        samtools fastq -1 {output.R1} -2 {output.R2} -0 /dev/null -n {input.bam}
        """


##############################
# Variant calling
##############################
rule picard_mark_duplicates:
    """Mark duplicates with picard"""
    output:
        bam="{roi}/{samplealias}/{srrun}.sort.dup.bam",
        metrics="{roi}/markdup/{samplealias}/{srrun}.sort.dup.dup_metrics.txt",
    input:
        bam="{roi}/{samplealias}/{srrun}.sort.bam",
    conda: "envs/environment.yml",
    benchmark: "benchmarks/picard_mark_duplicates/{roi}/{samplealias}/{srrun}.sort.dup.bam.benchmark.txt",
    log: "logs/picard_mark_duplicates/{roi}/{samplealias}/{srrun}.sort.dup.bam.log",
    threads: 1
    shell:
        """
        picard MarkDuplicates \
            --CREATE_INDEX true \
            --INPUT {input.bam} \
            --METRICS_FILE {output.metrics} \
            --OUTPUT {output.bam} > {log} 2>&1
        """


rule gatk_haplotypecaller:
    """Run GATK HaplotypeCaller"""
    output:
        vcf="{roi}{sep}{samplealias}/{srrun}.{label}{raw}.hc{mode}.vcf.gz",
        tbi="{roi}{sep}{samplealias}/{srrun}.{label}{raw}.hc{mode}.vcf.gz.tbi",
    input:
        bam="{roi}{sep}{samplealias}/{srrun}.{label}.bam",
        ref=os.path.join("{roi}", config["reference"]),
        dict=get_sequence_dictionary,
        fai=get_sequence_fai,
    params:
        mode_options=lambda wildcards: "-ERC GVCF" if wildcards.mode == ".g" else "",
        options=" ".join(["-A", "FisherStrand", "-A", "QualByDepth",
                          "-A", "MappingQuality", "-G", "StandardAnnotation"]),
    wildcard_constraints:
        mode="(.g|)",
        raw="(.raw|)",
    conda: "envs/environment.yml",
    benchmark: "benchmarks/gatk_haplotypecaller/{roi}{sep}{samplealias}/{srrun}.{label}{raw}.hc{mode}.vcf.gz.benchmark.txt",
    log: "logs/gatk_haplotypecaller/{roi}{sep}{samplealias}/{srrun}.{label}{raw}.hc{mode}.vcf.gz.log",
    threads: 1
    shell:
        """
        gatk HaplotypeCaller -OVI true {params.options} {params.mode_options} --input {input.bam} --output {output.vcf} --reference {input.ref} > {log} 2>&1
        """

rule gatk_raw_or_bqsr_variant_filtration:
    """Filter raw or bqsr variants"""
    output:
        vcf="{roi}{sep}{samplealias}/{srrun}.{label}{raw}.hc{mode}.filtered.vcf.gz",
        tbi="{roi}{sep}{samplealias}/{srrun}.{label}{raw}.hc{mode}.filtered.vcf.gz.tbi",
    input:
        vcf="{roi}{sep}{samplealias}/{srrun}.{label}{raw}.hc{mode}.vcf.gz",
    wildcard_constraints:
        raw="(.raw|.bqsr)"
    params:
        options=gatk_raw_or_bqsr_variant_filtration_options,
    conda: "envs/environment.yml",
    benchmark: "benchmarks/gatk_raw_variant_filtration/{roi}{sep}{samplealias}/{srrun}.{label}{raw}.hc{mode}.filter.vcf.gz.benchmark.txt",
    log: "logs/gatk_raw_variant_filtration/{roi}{sep}{samplealias}/{srrun}.{label}{raw}.hc{mode}.filter.vcf.gz.log",
    threads: 1
    shell:
        """
        gatk VariantFiltration -OVI true --variant {input.vcf} --output {output.vcf} {params.options} > {log} 2>&1
        """

rule gatk_base_recalibrator:
    """Recalibrate bases using raw variant calls as known sites"""
    output:
        table="{roi}{sep}{samplealias}/{srrun}.{label}.recal.table",
    input:
        bam="{roi}{sep}{samplealias}/{srrun}.{label}.bam",
        known="{roi}{sep}{samplealias}/{srrun}.{label}.raw.hc.g.filtered.vcf.gz",
        ref=os.path.join("{roi}", config["reference"])
    conda: "envs/environment.yml",
    benchmark: "benchmarks/gatk_base_recalibrator/{roi}{sep}{samplealias}/{srrun}.{label}.recal.bam.benchmark.txt",
    log: "logs/gatk_base_recalibrator/{roi}{sep}{samplealias}/{srrun}.{label}.recal.bam.log",
    threads: 1
    shell:
        """
        gatk BaseRecalibrator -I {input.bam} -R {input.ref} --known-sites {input.known} -O {output.table} > {log} 2>&1
        """

rule gatk_apply_bqsr:
    """Apply BQSR on input bam"""
    output:
        recal="{roi}{sep}{samplealias}/{srrun}.{label}.recal.bam",
        bai="{roi}{sep}{samplealias}/{srrun}.{label}.recal.bai",
    input:
        table="{roi}{sep}{samplealias}/{srrun}.{label}.recal.table",
        bam="{roi}{sep}{samplealias}/{srrun}.{label}.bam",
    conda: "envs/environment.yml",
    benchmark: "benchmarks/gatk_bqsr/{roi}{sep}{samplealias}/{srrun}.{label}.recal.output.txt",
    log: "logs/gatk_bqsr/{roi}{sep}{samplealias}/{srrun}.{label}.recal.log",
    threads: 1
    shell:
        """
        gatk ApplyBQSR -bqsr {input.table} -I {input.bam} -O {output.recal} > {log} 2>&1
        """


rule gatk_combine_gvcfs:
    """Run GATK CombineGVCFs"""
    output:
        vcf = "{roi}{sep}combine.g.vcf.gz",
        tbi = "{roi}{sep}combine.g.vcf.gz.tbi",
    input:
        vcf = gatk_combine_gvcfs_input,
        ref = os.path.join("{roi}", config["reference"])
    params:
        vcf = lambda wildcards, input: " ".join([f"-V {x}" for x in input.vcf])
    conda: "envs/environment.yml",
    benchmark: "benchmarks/gatk_combine_gvcfs/{roi}{sep}combined.g.vcf.gz.benchmark.txt",
    log: "logs/gatk_combine_gvcfs/{roi}{sep}combined.g.vcf.gz.log",
    threads: 1
    shell:
        """
        gatk CombineGVCFs -OVI true --output {output.vcf} --reference {input.ref} {params.vcf} > {log} 2>&1
        """

rule gatk_genotype_gvcfs:
    """GATK GenotypeGVCFs"""
    output:
        vcf = "{roi}{sep}allsites.vcf.gz",
        tbi = "{roi}{sep}allsites.vcf.gz.tbi",
    input:
        vcf = "{roi}{sep}combine.g.vcf.gz",
        ref = os.path.join("{roi}", config["reference"]),
    conda: "envs/environment.yml",
    benchmark: "benchmarks/gatk_genotype_gvcfs/{roi}{sep}allsites.vcf.gz.benchmark.txt",
    log: "logs/gatk_genotype_gvcfs/{roi}{sep}allsites.log",
    threads: 1
    shell:
        """
        gatk GenotypeGVCFs -OVI true -R {input.ref} -V {input.vcf} -O {output.vcf} --all-sites > {log} 2>&1
        """

##############################
# Quality control
##############################
rule fastqc:
    """FastQC"""
    output:
        txt="{roi}/fastqc/{samplealias}/{srrun}_{read}_fastqc/summary.txt",
    input:
        fastq="{roi}/{samplealias}/{srrun}_{read}.fastq.gz",
    wildcard_constraints:
        read="(R1|R2)"
    conda: "envs/environment.yml"
    benchmark: "benchmarks/fastqc/{roi}/{samplealias}/{srrun}_{read}.fastq.gz.html.benchmark.txt",
    log: "logs/fastqc/{roi}/{samplealias}/{srrun}_{read}.fastq.gz.html.log",
    threads: 1
    shell:
        """
        mkdir -p {wildcards.roi}/fastqc/{wildcards.samplealias}
        fastqc --extract -o {wildcards.roi}/fastqc/{wildcards.samplealias} {input.fastq} > {log} 2>&1
        """


rule qualimap_bamqc:
    """Run qualimap bamqc on bam files"""
    output:
        txt = "{roi}/qualimap/{prefix}_stats/genome_results.txt",
    input:
        bam = "{roi}/{prefix}.bam",
    conda: "envs/environment.yml"
    benchmark: "benchmarks/qualimap_bamqc/{roi}/{prefix}_stats/genome_results.txt.benchmark.txt",
    log: "logs/qualimap_bamqc/{roi}/{prefix}_stats/genome_results.txt.log",
    threads: 1
    shell:
        """
        qualimap bamqc -bam {input.bam} -outdir {wildcards.roi}/qualimap/{wildcards.prefix}_stats > {log} 2>&1
        """


rule multiqc_roi:
    """Make multiqc report for roi"""
    output:
        html="{roi}/multiqc_report.html",
    input:
        multiqc_roi_input,
    conda: "envs/environment.yml",
    benchmark: "benchmarks/multiqc_roi/{roi}/multiqc_report.html.benchmark.txt",
    log: "logs/multiqc_roi/{roi}/multiqc_report.html.log",
    threads: 1
    shell:
        """
        multiqc -f {wildcards.roi} --outdir {wildcards.roi} > {log} 2>&1
        """

##############################
# Make save script
##############################
rule make_roi_save_script:
    """Make roi save script"""
    output:
        sh="{roi}/save.sh"
    input:
        lambda wildcards: os.path.join(wildcards.roi, "allsites.vcf.gz")
    conda: "envs/environment.yml",
    params:
        ubam = lambda wildcards:  '--include \"*.unmapped.bam*\"' if config["output"][wildcards.roi]["ubam"] else "",
        fastq = lambda wildcards:  '--include \"*_R[12].fastq.gz\"' if config["output"][wildcards.roi]["fastq"] else "",
        recal_gvcf = lambda wildcards: f'--include \"*.recal.hc.g.vcf.gz*\"' if config["output"][wildcards.roi]["recal.gvcf"] else "",
        recal_bam = lambda wildcards: f'--include \"*.recal.ba[mi]\"' if config["output"][wildcards.roi]["recal.bam"] else "",
        reference = lambda wildcards:  f'--include \"{config["reference"]}\"' if config["output"][wildcards.roi]["reference"] else "",
        outdir = lambda wildcards: config["output"][wildcards.roi]["relpath"]
    log: "logs/{roi}/save.log"
    threads: 1
    shell:
        """
        echo "#/bin/bash" > {output.sh};
        echo 'rsync -av --include=\"*/\" {params.ubam}\
            {params.fastq} {params.reference} {params.recal_gvcf} {params.recal_bam}\
            --include *allsites.vcf.gz* --include *combine.g.vcf.gz* \
            --exclude=\"*\" {wildcards.roi} {params.outdir}' >> {output.sh}
        """


ruleorder: make_ubam_for_roi > samtools_index_bam

localrules: download_sraruninfo
