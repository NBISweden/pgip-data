"""Download and remap sequence data from an SRA bioproject

Currently the workflow reads a configuration file that contains
information about bioproject, reference, and more. See
../config/config.yaml for an example. The current format also supports
the definition of regions of interest, from which mapped reads will be
extracted.

Monkeyflower examples have been generated with command

    snakemake -j 14 --profile workflow/local

The workflow/local directory holds a configuration file for a
snakemake profile that sets some useful snakemake options.

"""
import os
import sys
import re
import pandas as pd
import shutil
from snakemake.remote.NCBI import RemoteProvider as NCBIRemoteProvider
from snakemake.remote.NCBI import NCBIFileException
from snakemake.remote.HTTP import RemoteProvider
from pathlib import Path
import logging

try:
    include: "custom.smk"
    custom_all=rules.custom_all.input
except Exception as e:
    print(e)
    custom_all=[]
    pass

configfile: "config/config.yaml"
workdir: config["bioproject"]

envvars:
    "EMAIL"
if "repeatlibrary" in config.keys():
    envvars:
        "REPEATMASKER_LIBDIR"

##############################
# Settings
##############################
# csvtk filter2
csvtk_filter2 = ""
if "sraruninfo" in config.keys():
    if "csvtk.filter2" in config["sraruninfo"].keys():
        csvtk_filter2 = config["sraruninfo"]["csvtk.filter2"]
        csvtk_filter2 = f"-f '{csvtk_filter2}'"
# Data sources
datasources = dict()
if "datasources" in config.keys():
    datasources = dict(zip(config["datasources"].keys(),
                           config["datasources"].values()))
if "repeatlibrary" not in config.keys():
    config["repeatlibrary"] = "repeats"


try:
    sraruninfo = pd.read_csv("SraRunInfo.csv")
except Exception as e:
    logging.error("No SraRunInfo.csv available yet! Rerun workflow once it has been downloaded")
    sraruninfo = pd.DataFrame(columns=["Run", "SampleName"])
    pass

NCBI = NCBIRemoteProvider(email=os.environ["EMAIL"])
HTTP = RemoteProvider()

try:
    REPEATMASKER_DIR=os.path.dirname(os.path.realpath(shutil.which("RepeatMasker")))
except Exception as e:
    raise

wildcard_constraints:
    roi="(|" + "|".join([x for x in config.get("output", {}).keys()]) + ")",
    sep="(|/)",
    srrid="(" + "|".join(sraruninfo.Run.values) + ")",
    sampleid="(" + "|".join(sraruninfo.SampleName.values) + ")",


all_results = dict(
    sampleinfo=["SraRunInfo.csv"],
    datasources=list(datasources.keys()),
    custom=custom_all,
    masked=expand(f'{config["reference"]}.{{suffix}}', suffix=["masked", "out", "tbl"]),
    srrbam=expand("{sampleid}/{srrid}.sort.md.bam.bai", zip, \
                  srrid=sraruninfo.Run.values, \
                  sampleid=sraruninfo.SampleName.values)
)


if "repeatlibrary" in config.keys():
    all_results["repeats"] = [f'{config["repeatlibrary"]}-families.fa']
    all_results["merged.lib"] = expand(
        f'{config["reference"]}.{config["repeatlibrary"]}.merged.{{suffix}}',
        suffix=["masked", "out", "tbl"]
    )

if "output" in config.keys():
    for roi, obj in config["output"].items():
        key = f"output.{roi}"
        results = [os.path.join(f"{roi}", "allsites.vcf.gz.tbi")]
        results.extend(expand(f"{roi}/{{sampleid}}/{{srrid}}_R1.fastq.gz",
                              zip, \
                              srrid=sraruninfo.Run.values, \
                              sampleid=sraruninfo.SampleName.values))
        results.extend(expand(f"{roi}/{{sampleid}}/{{srrid}}_R2.fastq.gz",
                              zip, \
                              srrid=sraruninfo.Run.values, \
                              sampleid=sraruninfo.SampleName.values))
        all_results[key] = results


rule all:
    """Pseudo-rule for all targets"""
    input: **all_results

##############################
# SRR module
#
# Download project data for a bioproject
#
##############################
rule download_sraruninfo:
    """Download sraruninfo for bioproject.

    config["sraruninfo"] configuration section contains configurations
    that apply to this rule.

    csvtk.filter2: set csvtk filter2 filtering parameter
    """
    output:
        "SraRunInfo.csv",
    benchmark: "benchmarks/SraRunInfo.csv.benchmark.txt",
    params:
        bioproject=config["bioproject"],
        filter2=csvtk_filter2,
    conda: "envs/sratools.yaml",
    log: "logs/SraRunInfo.csv.log",
    threads: 1
    shell:
        """
        esearch -db sra -query '{params.bioproject}' |
            efetch -format runinfo |
            csvtk filter2 {params.filter2} > {output}
        """


rule download_datasources:
    """Download datasources listed in config file using wget"""
    output:
        urltarget="{urltarget}",
    input:
        urlsource=lambda wildcards: HTTP.remote(config.get("datasources", {}).get(wildcards.urltarget))
    wildcard_constraints:
        urltarget=f'({"|".join([str(x) for x in set(datasources.keys())])})'
    conda: "envs/environment.yml",
    benchmark: "benchmarks/{urltarget}.benchmark.txt",
    log: "logs/{urltarget}.log",
    threads: 1
    shell:
        """
        wget {input.urlsource} -O {output.urltarget}
        """

##############################
# Reference indexing
##############################
rule samtools_faidx:
    """Run samtools faidx"""
    output: "{prefix}.fasta.fai",
    input: "{prefix}.fasta",
    conda: "envs/environment.yml",
    benchmark: "benchmarks/{prefix}.fasta.fai.benchmark.txt",
    log: "logs/{prefix}.fasta.fai.log",
    threads: 1
    shell:
        """samtools faidx {input}"""

rule picard_create_sequence_dictionary:
    """Create sequence dictionary"""
    output: "{prefix}.dict",
    input: "{prefix}.fasta",
    conda: "envs/environment.yml",
    benchmark: "benchmarks/{prefix}.dict.benchmark.txt",
    log: "logs/{prefix}.dict.log",
    threads: 1
    shell:
        """picard CreateSequenceDictionary --REFERENCE {input} > {log} 2>&1"""


##############################
# Repeat masking
##############################
rule build_database:
    """Build repeatmasker database"""
    output:
        lib=expand("{lib}.{sfx}", lib=config["repeatlibrary"],
                   sfx=["nhr", "nin", "njs", "nnd",
                        "nni", "nog", "nsq",
                        "translation"]),
    input: config["reference"],
    conda: "envs/repeatmasker.yml",
    benchmark: "benchmarks/repeats.benchmark.txt",
    log: "logs/repeats.builddatabase.log",
    threads: 1
    shell:
        """
        BuildDatabase -name {output} -engine ncbi {input} > {log} 2>&1
        """

rule repeat_modeler:
    """Run repeatmodeler"""
    output:
        fa="{lib}-families.fa",
        stk="{lib}-families.stk",
    input:
        lib="{lib}.nhr"
    wildcard_constraints:
        lib=config["repeatlibrary"]
    params:
        lib=config["repeatlibrary"],
    conda: "envs/repeatmasker.yml",
    benchmark: "benchmarks/repeat_modeler.{lib}.benchmark.txt"
    log: "logs/repeat_modeler.{lib}.log"
    threads: 12
    shell:
        """
        RepeatModeler -database {params.lib} -threads {threads} > {log} 2>&1
        """

rule repeat_masker_get_dfam_curated:
    """Install dfam_curated library

    One has to manually download and link a dfam library in the
    RepeatMasker installation location. The RepeatMasker directory is
    located at $(dirname $(readlink -f $(which RepeatMasker))). The
    dfam curated library is small enough to serve the purposes of this
    example.

        wget https://www.dfam.org/releases/Dfam_3.7/families/Dfam_curatedonly.h5.gz -O $(dirname $(readlink -f $(which RepeatMasker)))
    """
    output:
        h5="Libraries/Dfam.h5"
    input:
        dfam_curated=HTTP.remote("https://www.dfam.org/releases/Dfam_3.7/families/Dfam_curatedonly.h5.gz")
    params:
        dfam="Libraries/dfam_curated.h5.gz",
        h5="Libraries/Dfam.h5",
    conda: "envs/repeatmasker.yml",
    benchmark: "benchmarks/repeat_masker/dfam_curated.benchmark.txt"
    log: "logs/repeat_masker/dfam_curated.log"
    threads: 1
    shell:
        """
        wget {input.dfam_curated} -O {params.dfam} > {log} 2>&1
        gzip -dv {params.dfam} >> {log} 2>&1
        mv {params.dfam} {params.h5}
        """

rule repeat_masker:
    """Run repeatmasker on reference with generic repeat library.
    """
    output:
        fasta=f'{config["reference"]}.masked',
        out=f'{config["reference"]}.out',
        tbl=f'{config["reference"]}.tbl',
    input:
        ref=config["reference"],
        dfam="Libraries/Dfam.h5",
    params:
        species=config.get("species", "nn"),
        libdir=os.environ.get("REPEATMASKER_LIBDIR")
    conda: "envs/repeatmasker.yml",
    benchmark: "benchmarks/repeat_masker/masked.fasta.benchmark.txt",
    log: "logs/repeat_masker/masked.fasta.log",
    threads: 14
    shell:
        """
        RepeatMasker -libdir Libraries -libdir {params.libdir}  -species "{params.species}" -gff -pa {threads} -a -xsmall {input.ref} > {log} 2>&1
        """

rule repeat_masker_custom:
    """Run repeatmasker on reference with custom repeat library"""
    output:
        fasta=f'{config["reference"]}.{{lib}}.masked',
        out=f'{config["reference"]}.{{lib}}.out',
        tbl=f'{config["reference"]}.{{lib}}.tbl'
    input:
        ref=f'{config["reference"]}.masked',
        lib="{lib}-families.fa",
    wildcard_constraints:
        lib=config["repeatlibrary"],
    params:
        species=config.get("species", "nn"),
        ref={config["reference"]},
    conda: "envs/repeatmasker.yml",
    benchmark: "benchmarks/repeat_masker/{lib}.masked.fasta.benchmark.txt",
    log: "logs/repeat_masker/{lib}.masked.fasta.log",
    threads: 14
    shell:
        """
        RepeatMasker -lib {input.lib} -gff -pa {threads} -a -xsmall {input.ref} > {log} 2>&1
        rename "s/{params.ref}.masked/{params.ref}.{wildcards.lib}/g" {input.ref}.*
        """

rule repeat_masker_combine:
    """Combine repeatmasker outputs"""
    output:
        out=f'{config["reference"]}.{{lib}}.merged.{{suffix}}',
    input:
        custom=f'{config["reference"]}.{{lib}}.{{suffix}}',
        rm=f'{config["reference"]}.{{suffix}}'
    wildcard_constraints:
        suffix = "(out|tbl|masked)"
    conda: "envs/repeatmasker.yml",
    benchmark: "benchmarks/repeat_masker/{lib}.merged.{suffix}.benchmark.txt",
    log: "logs/repeat_masker/{lib}.merged.{suffix}.log",
    threads: 1
    shell:
        """
        cat {input.rm} {input.custom} > {output.out}
        """

##############################
# Mapping
##############################
BWA_INDEX_SUFFIX=["amb", "ann", "bwt", "pac", "sa"]
rule bwa_index:
    """Create bwa index for input file"""
    output:
        index=expand("{{prefix}}.fasta.{suffix}", suffix=BWA_INDEX_SUFFIX)
    input:
        fasta="{prefix}.fasta",
    conda: "envs/environment.yml",
    benchmark: "benchmarks/{prefix}.fasta.bwa_index.benchmark.txt",
    log: "logs/{prefix}.fasta.bwa_index.log",
    threads: 1
    shell:
        """bwa index {input.fasta} > {log} 2>&1"""

rule sra_prefetch:
    """Prefetch sra record. Note that output name is determined by
    prefetch."""
    output:
        srrid=temp("{srrid}/{srrid}.sra"),
    conda: "envs/environment.yml",
    benchmark: "benchmarks/sra_prefetch/{srrid}.benchmark.txt",
    log: "logs/sra_prefetch/{srrid}.log",
    threads: 1
    shell:
        """
        prefetch -p {output.srrid} > {log} 2>&1
        """

def get_read_group(wildcards):
    run=wildcards.srrid
    sample = sraruninfo[sraruninfo["Run"] == run].Sample.values[0]
    rg = f'-R "@RG\\tID:{wildcards.srrid}\\tSM:{sample}\\tPL:ILLUMINA"',
    return rg


rule bwa_mem_srrinput:
    """Map SRR data on the fly and convert to bam"""
    output:
        bam="{sampleid}/{srrid}.sort.md.bam",
    input:
        srr="{srrid}/{srrid}.sra",
        reference=config["reference"],
        index=expand("{ref}.{sfx}", ref=config["reference"], sfx=BWA_INDEX_SUFFIX),
    params:
        rg=get_read_group,
        options="-M"
    conda: "envs/environment.yml",
    benchmark: "benchmarks/{sampleid}/{srrid}.sort.md.bam.benchmark.txt",
    log: "logs/{sampleid}/{srrid}.sort.md.bam.log",
    threads: 14
    shell:
        """
        fasterq-dump --skip-technical -Z --split-spot {input.srr} |
            bwa mem {params.rg} -p -t {threads} {params.options} {input.reference} - |
            samtools fixmate -m - /dev/stdout |
            samtools sort - | samtools markdup - /dev/stdout |
            samtools view -h -b -o {output.bam} 2> {log}
        """

rule bwa_mem_ubam_to_roi:
    """Map ubam file to roi"""
    output:
        bam="{roi}/{sampleid}/{srrid}.sort.bam",
    input:
        reference=os.path.join("{roi}", config['reference']),
        index=expand(os.path.join("{{roi}}", "{ref}.{sfx}"), ref=config["reference"], sfx=BWA_INDEX_SUFFIX),
        bam="{roi}/{sampleid}/{srrid}.unmapped.bam",
        bai="{roi}/{sampleid}/{srrid}.unmapped.bam.bai",
    params:
        rg=get_read_group,
        options="-M"
    conda: "envs/environment.yml",
    benchmark: "benchmarks/bwa_mem_roi/{roi}/{sampleid}/{srrid}.bam.benchmark.txt",
    log: "logs/bwa_mem_roi/{roi}/{sampleid}/{srrid}.bam.log",
    threads: 14
    shell:
        """
            samtools fastq {input.bam} 2> {log}|
            bwa mem {params.rg} -p -t {threads} {params.options} {input.reference} - 2>> {log} |
            samtools fixmate -m - /dev/stdout |
            samtools sort - |
            samtools view -h -b -o {output.bam} >> {log} 2>&1
        """


rule samtools_index_bam:
    """Index bam file"""
    output: "{roi}{sep}{sampleid}/{srrid}.{tag}.bam.bai",
    input: "{roi}{sep}{sampleid}/{srrid}.{tag}.bam",
    conda: "envs/environment.yml",
    benchmark: "benchmarks/samtools_index_bam/{roi}{sep}{sampleid}/{srrid}.{tag}.bam.bai.benchmark.txt",
    log: "logs/samtools_index_bam/{srrid}/{roi}{sep}{srrid}.{sampleid}.{tag}.bam.bai.log",
    threads: 1
    shell:
        """
        samtools index {input} > {log} 2>&1
        """


##############################
# Region-based analyses (roi)
#
# 1. Subset reference to roi
#
# 2. Subset bam to roi
#
# 3. Make ubam
#
# 4. Remap ubam to reference
##############################
rule make_roi_bed:
    """Make roi bedfile"""
    output: "{roi}/{roi}.bed",
    params:
        roi = lambda wildcards: "\n".join(["\t".join(re.split(":|-", x)) for x in config["output"][wildcards.roi]["roi"]])
    conda: "envs/environment.yml",
    benchmark: "benchmarks/make_roi_bed/{roi}.bed.benchmark.txt",
    log: "logs/make_roi_bed/{roi}.bed.log",
    threads: 1
    shell:
        """
        echo -e "{params.roi}" > {output}
        """

rule subset_reference_to_roi:
    """Subset reference sequence to roi"""
    output:
        fasta=os.path.join("{roi}", config["reference"]),
    input:
        fasta=config["reference"],
        fai=config["reference"] + ".fai",
        bed="{roi}/{roi}.bed"
    conda: "envs/environment.yml",
    benchmark: "benchmarks/subset_reference_to_roi/{roi}.benchmark.txt",
    log: "logs/subset_reference_to_roi/{roi}.log",
    threads: 1
    shell:
        """
        seqtk subseq -l 60 {input.fasta} {input.bed} > {output.fasta} 2> {log}
        """

rule subset_bam_for_roi:
    """Subset bam file for roi"""
    output:
        bam="{roi}/{sampleid}/{srrid}.roi.bam",
    input:
        bam="{sampleid}/{srrid}.sort.md.bam",
        bai="{sampleid}/{srrid}.sort.md.bam.bai",
    params:
        roi=lambda wildcards: " ".join([f'"{x}"' for x in config["output"][wildcards.roi]["roi"]])
    conda: "envs/environment.yml",
    benchmark: "benchmarks/subset_bam_for_roi/{roi}/{sampleid}/{srrid}.roi.bam.benchmark.txt",
    log: "logs/subset_bam_for_roi/{roi}/{sampleid}/{srrid}.roi.bam.log",
    threads: 1
    shell:
        """
        samtools view -P {input.bam} {params.roi} -h -b -o {output.bam} > {log} 2>&1
        """

rule make_ubam_for_roi:
    """Make ubam file for roi"""
    output:
        bam="{roi}/{sampleid}/{srrid}.unmapped.bam",
    input:
        bam="{roi}/{sampleid}/{srrid}.roi.bam",
    conda: "envs/environment.yml",
    benchmark: "benchmarks/make_ubam_for_roi/{roi}/{sampleid}/{srrid}.unmapped.bam.benchmark.txt",
    log: "logs/make_ubam_for_roi/{roi}/{sampleid}/{srrid}.unmapped.bam.log",
    threads: 1
    shell:
        """
        picard RevertSam --INPUT {input.bam} \
                   --OUTPUT {output.bam} \
                   --SANITIZE true \
                   --MAX_DISCARD_FRACTION 0.5 \
                   --ATTRIBUTE_TO_CLEAR XT \
                   --ATTRIBUTE_TO_CLEAR XN \
                   --ATTRIBUTE_TO_CLEAR AS \
                   --ATTRIBUTE_TO_CLEAR OC \
                   --ATTRIBUTE_TO_CLEAR OP \
                   --SORT_ORDER queryname \
                   --RESTORE_ORIGINAL_QUALITIES true \
                   --REMOVE_DUPLICATE_INFORMATION true \
                   --REMOVE_ALIGNMENT_INFORMATION true \
                   > {log} 2>&1
        """

rule samtools_fastq_roi:
    """Make fastq files from roi"""
    output:
        R1="{roi}/{sampleid}/{srrid}_R1.fastq.gz",
        R2="{roi}/{sampleid}/{srrid}_R2.fastq.gz",
    input:
        bam="{roi}/{sampleid}/{srrid}.unmapped.bam",
        bai="{roi}/{sampleid}/{srrid}.unmapped.bam.bai",
    conda: "envs/environment.yml",
    benchmark: "benchmarks/samtools_fastq_roi/{roi}/{sampleid}/{srrid}.fastq.gz.benchmark.txt",
    log: "logs/samtools_fastq_roi/{roi}/{sampleid}/{srrid}.fastq.gz.log",
    threads: 1
    shell:
        """
        samtools fastq -1 {output.R1} -2 {output.R2} -0 /dev/null -n {input.bam}
        """


##############################
# Variant calling
##############################
rule picard_mark_duplicates:
    """Mark duplicates with picard"""
    output:
        bam="{roi}/{sampleid}/{srrid}.sort.dup.bam",
        metrics="{roi}/{sampleid}/{srrid}.sort.dup.dup_metrics.txt",
    input:
        bam="{roi}/{sampleid}/{srrid}.sort.bam",
    conda: "envs/environment.yml",
    benchmark: "benchmarks/picard_mark_duplicates/{roi}/{sampleid}/{srrid}.sort.dup.bam.benchmark.txt",
    log: "logs/picard_mark_duplicates/{roi}/{sampleid}/{srrid}.sort.dup.bam.log",
    threads: 1
    shell:
        """
        picard MarkDuplicates \
            --CREATE_INDEX true \
            --INPUT {input.bam} \
            --METRICS_FILE {output.metrics} \
            --OUTPUT {output.bam}
        """


rule gatk_haplotypecaller:
    """Run GATK HaplotypeCaller"""
    output:
        vcf="{roi}{sep}{sampleid}/{srrid}.{label}{raw}.hc{mode}.vcf.gz",
        tbi="{roi}{sep}{sampleid}/{srrid}.{label}{raw}.hc{mode}.vcf.gz.tbi",
    input:
        bam="{roi}{sep}{sampleid}/{srrid}.{label}.bam",
        ref=os.path.join("{roi}", config["reference"]),
        dict=re.sub(".fasta$", ".dict", os.path.join("{roi}", config["reference"])),
        fai=os.path.join("{roi}", config["reference"] + ".fai"),
    params:
        mode_options=lambda wildcards: "-ERC GVCF" if wildcards.mode == ".g" else "",
        options=" ".join(["-A", "FisherStrand", "-A", "QualByDepth",
                          "-A", "MappingQuality", "-G", "StandardAnnotation",
                          "--minimum-mapping-quality", "50"]),
    wildcard_constraints:
        mode="(.g|)",
        raw="(.raw|)",
    conda: "envs/environment.yml",
    benchmark: "benchmarks/gatk_haplotypecaller/{roi}{sep}{sampleid}/{srrid}.{label}{raw}.hc{mode}.vcf.gz.benchmark.txt",
    log: "logs/gatk_haplotypecaller/{roi}{sep}{sampleid}/{srrid}.{label}{raw}.hc{mode}.vcf.gz.log",
    threads: 1
    shell:
        """
        gatk HaplotypeCaller -OVI true {params.options} {params.mode_options} --input {input.bam} --output {output.vcf} --reference {input.ref}
        """

def gatk_raw_or_bqsr_variant_filtration_options(wildcards):
    if wildcards.raw == ".raw":
        options = [
            "--filter-name", "FisherStrand",
            "--filter", "'FS > 50.0'",
            "--filter-name", "QualByDepth",
            "--filter", "'QD < 4.0'",
            "--filter-name", "MappingQuality",
            "--filter", "'MQ < 50.0'"
        ]
    else:
        options = [
            "--filter-name", "FisherStrand",
            "--filter", "'FS > 60.0'",
            "--filter-name", "QualByDepth",
            "--filter", "'QD < 2.0'",
            "--filter-name", "MappingQuality",
            "--filter", "'MQ < 40.0'"
        ]

    return " ".join(options)

rule gatk_raw_or_bqsr_variant_filtration:
    """Filter raw or bqsr variants"""
    output:
        vcf="{roi}{sep}{sampleid}/{srrid}.{label}{raw}.hc{mode}.filtered.vcf.gz",
        tbi="{roi}{sep}{sampleid}/{srrid}.{label}{raw}.hc{mode}.filtered.vcf.gz.tbi",
    input:
        vcf="{roi}{sep}{sampleid}/{srrid}.{label}{raw}.hc{mode}.vcf.gz",
    wildcard_constraints:
        raw="(.raw|.bqsr)"
    params:
        options=gatk_raw_or_bqsr_variant_filtration_options,
    conda: "envs/environment.yml",
    benchmark: "benchmarks/gatk_raw_variant_filtration/{roi}{sep}{sampleid}/{srrid}.{label}{raw}.hc{mode}.filter.vcf.gz.benchmark.txt",
    log: "logs/gatk_raw_variant_filtration/{roi}{sep}{sampleid}/{srrid}.{label}{raw}.hc{mode}.filter.vcf.gz.log",
    threads: 1
    shell:
        """
        gatk VariantFiltration -OVI true --variant {input.vcf} --output {output.vcf} {params.options}
        """

rule gatk_base_recalibrator:
    """Recalibrate bases using raw variant calls as known sites"""
    output:
        table="{roi}{sep}{sampleid}/{srrid}.{label}.recal.table",
    input:
        bam="{roi}{sep}{sampleid}/{srrid}.{label}.bam",
        known="{roi}{sep}{sampleid}/{srrid}.{label}.raw.hc.g.filtered.vcf.gz",
        ref=os.path.join("{roi}", config["reference"])
    conda: "envs/environment.yml",
    benchmark: "benchmarks/gatk_base_recalibrator/{roi}{sep}{sampleid}/{srrid}.{label}.recal.bam.benchmark.txt",
    log: "logs/gatk_base_recalibrator/{roi}{sep}{sampleid}/{srrid}.{label}.recal.bam.log",
    threads: 1
    shell:
        """
        gatk BaseRecalibrator -I {input.bam} -R {input.ref} --known-sites {input.known} -O {output.table}
        """

rule gatk_apply_bqsr:
    """Apply BQSR on input bam"""
    output:
        recal="{roi}{sep}{sampleid}/{srrid}.{label}.recal.bam",
    input:
        table="{roi}{sep}{sampleid}/{srrid}.{label}.recal.table",
        bam="{roi}{sep}{sampleid}/{srrid}.{label}.bam",
    conda: "envs/environment.yml",
    benchmark: "benchmarks/gatk_bqsr/{roi}{sep}{sampleid}/{srrid}.{label}.recal.output.txt",
    log: "logs/gatk_bqsr/{roi}{sep}{sampleid}/{srrid}.{label}.recal.log",
    threads: 1
    shell:
        """
        gatk ApplyBQSR -bqsr {input.table} -I {input.bam} -O {output.recal}
        """


rule gatk_combine_gvcfs:
    """Run GATK CombineGVCFs"""
    output:
        vcf = "{roi}{sep}combine.g.vcf.gz",
        tbi = "{roi}{sep}combine.g.vcf.gz.tbi",
    input:
        vcf = expand("{{roi}}{{sep}}{sampleid}/{srrid}.sort.dup.recal.hc.g.vcf.gz", zip, \
                     sampleid=sraruninfo.SampleName.values, \
                     srrid=sraruninfo.Run.values),
        ref = os.path.join("{roi}", config["reference"])
    params:
        vcf = lambda wildcards, input: " ".join([f"-V {x}" for x in input.vcf])
    conda: "envs/environment.yml",
    benchmark: "benchmarks/gatk_combine_gvcfs/{roi}{sep}all.combined.g.vcf.gz.benchmark.txt",
    log: "logs/gatk_combine_gvcfs/{roi}{sep}all.combined.g.vcf.gz.log",
    threads: 1
    shell:
        """
        gatk CombineGVCFs -OVI true --output {output.vcf} --reference {input.ref} {params.vcf}
        """

rule gatk_genotype_gvcfs:
    """GATK GenotypeGVCFs"""
    output:
        vcf = "{roi}{sep}allsites.vcf.gz",
        tbi = "{roi}{sep}allsites.vcf.gz.tbi",
    input:
        vcf = "{roi}{sep}combine.g.vcf.gz",
        ref = os.path.join("{roi}", config["reference"]),
    conda: "envs/environment.yml",
    benchmark: "benchmarks/gatk_genotype_gvcfs/{roi}{sep}allsites.vcf.gz.benchmark.txt",
    log: "logs/gatk_genotype_gvcfs/{roi}{sep}allsites.log",
    threads: 1
    shell:
        """
        gatk GenotypeGVCFs -OVI true -R {input.ref} -V {input.vcf} -O {output.vcf} --all-sites
        """




##############################
# Make save script
##############################
def make_roi_save_script_input(wildcards):
    roi = config["output"][wildcards.roi]
    data = []
    if roi["ubam"]:
        data.extend(
            expand(f"{wildcards.roi}/{{srrid}}/{{srrid}}.{{sampleid}}.unmapped.bam",
                   zip,
                   srrid=sraruninfo.Run.values,
                   sampleid=sraruninfo.SampleName.values)
        )
        data.extend(
            expand(f"{wildcards.roi}/{{srrid}}/{{srrid}}.{{sampleid}}.unmapped.bam.bai",
                   zip,
                   srrid=sraruninfo.Run.values,
                   sampleid=sraruninfo.SampleName.values)
        )
    if roi["fastq"]:
        data.extend(
            expand(f"{wildcards.roi}/{{srrid}}/{{srrid}}.{{sampleid}}_R1.fastq.gz",
                   zip,
                   srrid=sraruninfo.Run.values,
                   sampleid=sraruninfo.SampleName.values)
        )
        data.extend(
            expand(f"{wildcards.roi}/{{srrid}}/{{srrid}}.{{sampleid}}_R2.fastq.gz",
                   zip,
                   srrid=sraruninfo.Run.values,
                   sampleid=sraruninfo.SampleName.values)
        )
    if roi["reference"]:
        data.append(os.path.join(wildcards.roi, config["reference"]))
    return data


rule make_roi_save_script:
    """Make roi save script"""
    output:
        sh="{roi}/save.sh"
    input:
        make_roi_save_script_input,
    params:
        ubam = lambda wildcards:  '--include \"*.unmapped.bam*\"' if config["output"][wildcards.roi]["ubam"] else "",
        fastq = lambda wildcards:  '--include \"*_R[12].fastq.gz\"' if config["output"][wildcards.roi]["fastq"] else "",
        reference = lambda wildcards:  f'--include \"{config["reference"]}\"' if config["output"][wildcards.roi]["reference"] else "",
        outdir = lambda wildcards: config["output"][wildcards.roi]["relpath"]
    threads: 1
    shell:
        """
        echo "#/bin/bash" > {output.sh};
        echo 'rsync -av --include=\"*/\" {params.ubam}\
            {params.fastq} {params.reference} \
            --exclude=\"*\" {wildcards.roi} {params.outdir}' >> {output.sh}
        """
